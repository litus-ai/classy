"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5969],{3905:(t,e,a)=>{a.d(e,{Zo:()=>c,kt:()=>u});var s=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function i(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(t);e&&(s=s.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,s)}return a}function l(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?i(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function n(t,e){if(null==t)return{};var a,s,r=function(t,e){if(null==t)return{};var a,s,r={},i=Object.keys(t);for(s=0;s<i.length;s++)a=i[s],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(t);for(s=0;s<i.length;s++)a=i[s],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var o=s.createContext({}),d=function(t){var e=s.useContext(o),a=e;return t&&(a="function"==typeof t?t(e):l(l({},e),t)),a},c=function(t){var e=d(t.components);return s.createElement(o.Provider,{value:e},t.children)},k={inlineCode:"code",wrapper:function(t){var e=t.children;return s.createElement(s.Fragment,{},e)}},_=s.forwardRef((function(t,e){var a=t.components,r=t.mdxType,i=t.originalType,o=t.parentName,c=n(t,["components","mdxType","originalType","parentName"]),_=d(a),u=r,f=_["".concat(o,".").concat(u)]||_[u]||k[u]||i;return a?s.createElement(f,l(l({ref:e},c),{},{components:a})):s.createElement(f,l({ref:e},c))}));function u(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var i=a.length,l=new Array(i);l[0]=_;var n={};for(var o in e)hasOwnProperty.call(e,o)&&(n[o]=e[o]);n.originalType=t,n.mdxType="string"==typeof t?t:r,l[1]=n;for(var d=2;d<i;d++)l[d]=a[d];return s.createElement.apply(null,l)}return s.createElement.apply(null,a)}_.displayName="MDXCreateElement"},7650:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>k,frontMatter:()=>i,metadata:()=>n,toc:()=>d});var s=a(7462),r=(a(7294),a(3905));const i={title:"classy.data.dataset.hf.classification",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},l=void 0,n={unversionedId:"api/data/dataset/hf/classification",id:"api/data/dataset/hf/classification",title:"classy.data.dataset.hf.classification",description:"Classes",source:"@site/docs/api/data/dataset/hf/classification.md",sourceDirName:"api/data/dataset/hf",slug:"/api/data/dataset/hf/classification",permalink:"/classy/docs/api/data/dataset/hf/classification",draft:!1,tags:[],version:"current",frontMatter:{title:"classy.data.dataset.hf.classification",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},sidebar:"apiSidebar"},o={},d=[{value:"Classes",id:"clzs",level:2},{value:"HFQADataset",id:"HFQADataset",level:3},{value:"__init__",id:"HFQADataset-init",level:4},{value:"dataset_iterator_func",id:"HFQADataset-dataset_iterator_func",level:4},{value:"fit_vocabulary",id:"HFQADataset-fit_vocabulary",level:4},{value:"requires_vocab",id:"HFQADataset-requires_vocab",level:4},{value:"HFSentencePairDataset",id:"HFSentencePairDataset",level:3},{value:"__init__",id:"HFSentencePairDataset-init",level:4},{value:"dataset_iterator_func",id:"HFSentencePairDataset-dataset_iterator_func",level:4},{value:"HFSequenceDataset",id:"HFSequenceDataset",level:3},{value:"__init__",id:"HFSequenceDataset-init",level:4},{value:"dataset_iterator_func",id:"HFSequenceDataset-dataset_iterator_func",level:4},{value:"fit_vocabulary",id:"HFSequenceDataset-fit_vocabulary",level:4},{value:"HFTokenDataset",id:"HFTokenDataset",level:3},{value:"__init__",id:"HFTokenDataset-init",level:4},{value:"dataset_iterator_func",id:"HFTokenDataset-dataset_iterator_func",level:4},{value:"tokenize",id:"HFTokenDataset-tokenize",level:4},{value:"fit_vocabulary",id:"HFTokenDataset-fit_vocabulary",level:4}],c={toc:d};function k(t){let{components:e,...a}=t;return(0,r.kt)("wrapper",(0,s.Z)({},c,a,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"clzs"},"Classes"),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"HFQADataset"},"HFQADataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"HFQADataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFQADataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L182-L296",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("h4",{id:"HFQADataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0transformer_model:\xa0str,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0additional_special_tokens:\xa0Optional[List[str]]\xa0=\xa0None,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0truncation:\xa0Union[bool,\xa0str]\xa0=\xa0False,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0max_length:\xa0int\xa0=\xa0-1,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFQADataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L182-L296",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFQADataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),") \u2011>\xa0Iterable[Dict[str,\xa0Any]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFQADataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L191-L279",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFQADataset-fit_vocabulary"},"fit_vocabulary"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"fit_vocabulary"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0samples:\xa0Iterator[",(0,r.kt)("a",{title:"QASample",href:"/docs/api/data/data_drivers#QASample"},"QASample"),"],",(0,r.kt)("br",null),") \u2011>\xa0",(0,r.kt)("a",{title:"Vocabulary",href:"/docs/api/utils/vocabulary#Vocabulary"},"Vocabulary"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFQADataset-fit_vocabulary",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L187-L189",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFQADataset-requires_vocab"},"requires_vocab"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"requires_vocab"),"() \u2011>\xa0bool",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFQADataset-requires_vocab",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L183-L185",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"HFSentencePairDataset"},"HFSentencePairDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"HFSentencePairDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSentencePairDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L137-L179",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("h4",{id:"HFSentencePairDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0transformer_model:\xa0str,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0additional_special_tokens:\xa0Optional[List[str]]\xa0=\xa0None,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0truncation:\xa0Union[bool,\xa0str]\xa0=\xa0False,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0max_length:\xa0int\xa0=\xa0-1,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSentencePairDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L137-L179",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFSentencePairDataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),") \u2011>\xa0Iterable[Dict[str,\xa0Any]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSentencePairDataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L146-L179",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"HFSequenceDataset"},"HFSequenceDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"HFSequenceDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSequenceDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L19-L62",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Subclasses (1)"),(0,r.kt)("div",null,(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"HFSentencePairDataset",href:"/docs/api/data/dataset/hf/classification#HFSentencePairDataset"},"HFSentencePairDataset"))))),(0,r.kt)("h4",{id:"HFSequenceDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0transformer_model:\xa0str,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0additional_special_tokens:\xa0Optional[List[str]]\xa0=\xa0None,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0truncation:\xa0Union[bool,\xa0str]\xa0=\xa0False,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0max_length:\xa0int\xa0=\xa0-1,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSequenceDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L19-L62",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFSequenceDataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),") \u2011>\xa0Iterable[Dict[str,\xa0Any]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSequenceDataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L39-L62",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFSequenceDataset-fit_vocabulary"},"fit_vocabulary"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"fit_vocabulary"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0samples:\xa0Iterator[",(0,r.kt)("a",{title:"SequenceSample",href:"/docs/api/data/data_drivers#SequenceSample"},"SequenceSample"),"],",(0,r.kt)("br",null),") \u2011>\xa0",(0,r.kt)("a",{title:"Vocabulary",href:"/docs/api/utils/vocabulary#Vocabulary"},"Vocabulary"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFSequenceDataset-fit_vocabulary",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L20-L24",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"HFTokenDataset"},"HFTokenDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"HFTokenDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFTokenDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L65-L134",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("h4",{id:"HFTokenDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0transformer_model:\xa0str,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0additional_special_tokens:\xa0Optional[List[str]]\xa0=\xa0None,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0truncation:\xa0Union[bool,\xa0str]\xa0=\xa0False,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0max_length:\xa0int\xa0=\xa0-1,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFTokenDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L65-L134",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFTokenDataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),") \u2011>\xa0Iterable[Dict[str,\xa0Any]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFTokenDataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L92-L113",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFTokenDataset-tokenize"},"tokenize"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"tokenize"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0tokens:\xa0List[str],",(0,r.kt)("br",null),") \u2011>\xa0Optional[Tuple[torch.Tensor,\xa0List[Tuple[int,\xa0int]]]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFTokenDataset-tokenize",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L115-L134",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFTokenDataset-fit_vocabulary"},"fit_vocabulary"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"fit_vocabulary"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0samples:\xa0Iterator[",(0,r.kt)("a",{title:"TokensSample",href:"/docs/api/data/data_drivers#TokensSample"},"TokensSample"),"],",(0,r.kt)("br",null),") \u2011>\xa0",(0,r.kt)("a",{title:"Vocabulary",href:"/docs/api/utils/vocabulary#Vocabulary"},"Vocabulary"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFTokenDataset-fit_vocabulary",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/classification.py#L66-L74",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))))}k.isMDXComponent=!0}}]);