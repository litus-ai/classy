"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9577],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>d});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=c(a),d=s,f=p["".concat(l,".").concat(d)]||p[d]||m[d]||r;return a?n.createElement(f,i(i({ref:t},u),{},{components:a})):n.createElement(f,i({ref:t},u))}));function d(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=a.length,i=new Array(r);i[0]=p;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:s,i[1]=o;for(var c=2;c<r;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},1917:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var n=a(7462),s=(a(7294),a(3905));const r={sidebar_position:5,title:"Custom Dataset"},i=void 0,o={unversionedId:"getting-started/customizing-things/custom-dataset",id:"getting-started/customizing-things/custom-dataset",title:"Custom Dataset",description:"Implementing your custom dataset with classy is easy. You just need to subclass BaseDataset:",source:"@site/docs/getting-started/customizing-things/custom-dataset.md",sourceDirName:"getting-started/customizing-things",slug:"/getting-started/customizing-things/custom-dataset",permalink:"/classy/docs/getting-started/customizing-things/custom-dataset",draft:!1,editUrl:"https://github.com/sunglasses-ai/classy/edit/main/docs/docs/getting-started/customizing-things/custom-dataset.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,title:"Custom Dataset"},sidebar:"tutorialSidebar",previous:{title:"Custom Model",permalink:"/classy/docs/getting-started/customizing-things/custom-model"},next:{title:"Custom Optimizer",permalink:"/classy/docs/getting-started/customizing-things/custom-optimizer"}},l={},c=[{value:"A Minimal Example",id:"a-minimal-example",level:2}],u={toc:c};function m(e){let{components:t,...a}=e;return(0,s.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"Implementing your custom dataset with classy is easy. You just need to subclass BaseDataset:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class MyCustomDataset(BaseDataset):\n    @staticmethod\n    def requires_vocab() -> bool:\n        # returns true if the dataset requires fitting a vocabulary, false otherwise\n        pass\n\n    @staticmethod\n    def fit_vocabulary(samples: Iterator[ClassySample]) -> Vocabulary:\n        # fits the vocabulary\n        pass\n\n    def __init__(self, *args, **kwargs):\n        # construct fields batchers\n        fields_batchers = {...}\n        super().__init__(*args, fields_batchers=fields_batchers, **kwargs)\n\n    def dataset_iterator_func(self) -> Iterable[Dict[str, Any]]:\n        # yields a sequence of dictionaries, each representing a sample\n        pass\n")),(0,s.kt)("p",null,"The underlying flow is as follows:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Dataset instantiation is transparent to you, and takes place via 1 of 3 class methods:",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"BaseDataset.from_file"),(0,s.kt)("li",{parentName:"ul"},"BaseDataset.from_lines"),(0,s.kt)("li",{parentName:"ul"},"BaseDataset.from_samples"))),(0,s.kt)("li",{parentName:"ul"},"Regardless on which one is invoked, BaseDataset exposes to you a ",(0,s.kt)("em",{parentName:"li"},"samples_iterator")," function that, once invoked, returns a sequence of classy samples"),(0,s.kt)("li",{parentName:"ul"},"In your ",(0,s.kt)("em",{parentName:"li"},"dataset_iterator_func"),", you iterate on these samples and convert them to dictionary objects"),(0,s.kt)("li",{parentName:"ul"},"These dictionary-like samples are then batched using the ",(0,s.kt)("em",{parentName:"li"},"fields_batchers")," variable you pass to BaseDataset in your ",(0,s.kt)("em",{parentName:"li"},"_","_","init","_","_"),"; it is essentially a dictionary mapping\nkeys in your dictionary-like samples to collating functions")),(0,s.kt)("h2",{id:"a-minimal-example"},"A Minimal Example"),(0,s.kt)("p",null,"Practically, imagine you want to build your own SequenceDataset for BERT."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="classy.data.dataset.my_bert_sequence_dataset.py"',title:'"classy.data.dataset.my_bert_sequence_dataset.py"'},"from transformers import AutoTokenizer\nfrom classy.data.data_drivers import SequenceSample\nfrom classy.data.dataset.base import batchify, BaseDataset\n\n\nclass MyBertSequenceDataset(BaseDataset):\n    pass\n")),(0,s.kt)("p",null,"You first deal with the vocabulary methods. As you are doing sequence classification, you need to fit the label vocabulary:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'@staticmethod\ndef requires_vocab() -> bool:\n    return True\n\n\n@staticmethod\ndef fit_vocabulary(samples: Iterator[SequenceSample]) -> Vocabulary:\n    return Vocabulary.from_samples(\n        [{"labels": sample.reference_annotation} for sample in samples]\n    )\n')),(0,s.kt)("p",null,"Then, define your constructor and, in particular, your ",(0,s.kt)("em",{parentName:"p"},"fields_batchers"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'def __init__(\n    self,\n    samples_iterator: Callable[[], Iterator[SequenceSample]],\n    vocabulary: Vocabulary,\n    transformer_model: str,\n    tokens_per_batch: int,\n    max_batch_size: Optional[int],\n    section_size: int,\n    prebatch: bool,\n    materialize: bool,\n    min_length: int,\n    max_length: int,\n    for_inference: bool,\n):\n\n    # load bert tokenizer\n    self.tokenizer = AutoTokenizer.from_pretrained(\n        transformer_model, use_fast=True, add_prefix_space=True\n    )\n\n    # define fields_batchers\n    fields_batcher = {\n        "input_ids": lambda lst: batchify(\n            lst, padding_value=self.tokenizer.pad_token_id\n        ),\n        "attention_mask": lambda lst: batchify(lst, padding_value=0),\n        "labels": lambda lst: torch.tensor(lst, dtype=torch.long),\n        "samples": None,\n    }\n\n    super().__init__(\n        samples_iterator=samples_iterator,\n        vocabulary=vocabulary,\n        batching_fields=["input_ids"],\n        tokens_per_batch=tokens_per_batch,\n        max_batch_size=max_batch_size,\n        fields_batchers=fields_batcher,\n        section_size=section_size,\n        prebatch=prebatch,\n        materialize=materialize,\n        min_length=min_length,\n        max_length=max_length if max_length != -1 else self.tokenizer.model_max_length,\n        for_inference=for_inference,\n    )\n')),(0,s.kt)("p",null,"Finally, you need to implement the ",(0,s.kt)("em",{parentName:"p"},"dataset_iterator_func"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'def dataset_iterator_func(self) -> Iterable[Dict[str, Any]]:\n    # iterate on samples\n    for sequence_sample in self.samples_iterator():\n        # invoke tokenizer\n        input_ids = self.tokenizer(sequence_sample.sequence, return_tensors="pt")[\n            "input_ids"\n        ][0]\n        # build dict\n        elem_dict = {\n            "input_ids": input_ids,\n            "attention_mask": torch.ones_like(input_ids),\n        }\n        if sequence_sample.reference_annotation is not None:\n            # use vocabulary to convert string labels to int labels\n            elem_dict["labels"] = [\n                self.vocabulary.get_idx(\n                    k="labels", elem=sequence_sample.reference_annotation\n                )\n            ]\n        elem_dict["samples"] = sequence_sample\n        yield elem_dict\n')))}m.isMDXComponent=!0}}]);