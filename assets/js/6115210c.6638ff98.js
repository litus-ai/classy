/*! For license information please see 6115210c.6638ff98.js.LICENSE.txt */
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4293],{3905:(t,e,a)=>{a.d(e,{Zo:()=>p,kt:()=>d});var n=a(7294);function i(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function r(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function s(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?r(Object(a),!0).forEach((function(e){i(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function l(t,e){if(null==t)return{};var a,n,i=function(t,e){if(null==t)return{};var a,n,i={},r=Object.keys(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||(i[a]=t[a]);return i}(t,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(i[a]=t[a])}return i}var o=n.createContext({}),c=function(t){var e=n.useContext(o),a=e;return t&&(a="function"==typeof t?t(e):s(s({},e),t)),a},p=function(t){var e=c(t.components);return n.createElement(o.Provider,{value:e},t.children)},u={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},m=n.forwardRef((function(t,e){var a=t.components,i=t.mdxType,r=t.originalType,o=t.parentName,p=l(t,["components","mdxType","originalType","parentName"]),m=c(a),d=i,h=m["".concat(o,".").concat(d)]||m[d]||u[d]||r;return a?n.createElement(h,s(s({ref:e},p),{},{components:a})):n.createElement(h,s({ref:e},p))}));function d(t,e){var a=arguments,i=e&&e.mdxType;if("string"==typeof t||i){var r=a.length,s=new Array(r);s[0]=m;var l={};for(var o in e)hasOwnProperty.call(e,o)&&(l[o]=e[o]);l.originalType=t,l.mdxType="string"==typeof t?t:i,s[1]=l;for(var c=2;c<r;c++)s[c]=a[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},856:(t,e,a)=>{a.d(e,{Z:()=>r});var n=a(7294);class i{constructor(t,e){void 0===t&&(t="#termynal"),void 0===e&&(e={}),this.container="string"==typeof t?document.querySelector(t):t,this.pfx=`data-${e.prefix||"ty"}`,this.originalStartDelay=this.startDelay=e.startDelay||parseFloat(this.container.getAttribute(`${this.pfx}-startDelay`))||600,this.originalTypeDelay=this.typeDelay=e.typeDelay||parseFloat(this.container.getAttribute(`${this.pfx}-typeDelay`))||90,this.originalLineDelay=this.lineDelay=e.lineDelay||parseFloat(this.container.getAttribute(`${this.pfx}-lineDelay`))||1500,this.progressLength=e.progressLength||parseFloat(this.container.getAttribute(`${this.pfx}-progressLength`))||40,this.progressChar=e.progressChar||this.container.getAttribute(`${this.pfx}-progressChar`)||"\u2588",this.progressPercent=e.progressPercent||parseFloat(this.container.getAttribute(`${this.pfx}-progressPercent`))||100,this.cursor=e.cursor||this.container.getAttribute(`${this.pfx}-cursor`)||"\u258b",this.lineData=this.lineDataToElements(e.lineData||[]),this.loadLines(),e.noInit||this.init()}loadLines(){const t=this.generateFinish();t.style.visibility="hidden",this.container.appendChild(t),this.lines=Array.from(this.container.querySelectorAll(`[${this.pfx}]`)).concat(this.lineData);for(let a=0;a<this.lines.length;a++)this.lines[a].style.visibility="hidden",this.container.appendChild(this.lines[a]);const e=this.generateRestart();e.style.visibility="hidden",this.container.appendChild(e),this.container.setAttribute("data-termynal","")}init(){const t=getComputedStyle(this.container);this.container.style.width="0px"!==t.width?t.width:void 0,this.container.style.minHeight="0px"!==t.height?t.height:void 0,this.container.setAttribute("data-termynal",""),this.container.innerHTML="";for(let e of this.lines)e.style.visibility="visible";this.start()}async start(){this.addFinish(),await this._wait(this.startDelay);for(let t of this.lines){const e=t.getAttribute(this.pfx),a=t.getAttribute(`${this.pfx}-delay`)||this.lineDelay;"input"===e?(t.setAttribute(`${this.pfx}-cursor`,this.cursor),await this.type(t),await this._wait(a)):"progress"===e?(await this.progress(t),await this._wait(a)):(this.container.appendChild(t),await this._wait(a)),t.removeAttribute(`${this.pfx}-cursor`)}this.addRestart(),this.finishElement.style.visibility="hidden",this.lineDelay=this.originalLineDelay,this.typeDelay=this.originalTypeDelay,this.startDelay=this.originalStartDelay}generateRestart(){const t=document.createElement("a");return t.onclick=t=>{t.preventDefault(),this.container.innerHTML="",this.init()},t.href="#",t.setAttribute("data-terminal-control",""),t.innerHTML="restart \u21bb",t}generateFinish(){const t=document.createElement("a");return t.onclick=t=>{t.preventDefault(),this.lineDelay=0,this.typeDelay=0,this.startDelay=0},t.href="#",t.setAttribute("data-terminal-control",""),t.innerHTML="fast \u2192",this.finishElement=t,t}addRestart(){const t=this.generateRestart();this.container.appendChild(t)}addFinish(){const t=this.generateFinish();this.container.appendChild(t)}async type(t){const e=Array.from(t.textContent);t.textContent="",this.container.appendChild(t);for(let a of e){const e=t.getAttribute(`${this.pfx}-typeDelay`)||this.typeDelay;await this._wait(e),t.textContent+=a}}async progress(t){const e=t.getAttribute(`${this.pfx}-progressLength`)||this.progressLength,a=(t.getAttribute(`${this.pfx}-progressChar`)||this.progressChar).repeat(e),n=t.getAttribute(`${this.pfx}-progressPercent`)||this.progressPercent;t.textContent="",this.container.appendChild(t);for(let i=1;i<a.length+1;i++){await this._wait(this.typeDelay);const e=Math.round(i/a.length*100);if(t.textContent=`${a.slice(0,i)} ${e}%`,e>n)break}}_wait(t){return new Promise((e=>setTimeout(e,t)))}lineDataToElements(t){return t.map((t=>{let e=document.createElement("div");return e.innerHTML=`<span ${this._attributes(t)}>${t.value||""}</span>`,e.firstElementChild}))}_attributes(t){let e="";for(let a in t)"class"!==a?"type"===a?e+=`${this.pfx}="${t[a]}" `:"value"!==a&&(e+=`${this.pfx}-${a}="${t[a]}" `):e+=` class=${t[a]} `;return e}}class r extends n.Component{componentDidMount(){new i(this.t,{typeDelay:40,lineDelay:700})}render(){return n.createElement("div",null,n.createElement("div",{"data-terminal":!0,style:this.props.style,ref:t=>this.t=t},this.props.children))}}},150:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>p});var n=a(7462),i=(a(7294),a(3905)),r=a(856);const s={sidebar_position:7,title:"Custom Evaluation Metric"},l=void 0,o={unversionedId:"getting-started/customizing-things/custom-metric",id:"getting-started/customizing-things/custom-metric",title:"Custom Evaluation Metric",description:"Adding a custom metric for evaluation is easy in classy, and you can use it for both classy evaluate and",source:"@site/docs/getting-started/customizing-things/custom-metric.md",sourceDirName:"getting-started/customizing-things",slug:"/getting-started/customizing-things/custom-metric",permalink:"/classy/docs/getting-started/customizing-things/custom-metric",draft:!1,editUrl:"https://github.com/sunglasses-ai/classy/edit/main/docs/docs/getting-started/customizing-things/custom-metric.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7,title:"Custom Evaluation Metric"},sidebar:"tutorialSidebar",previous:{title:"Custom Optimizer",permalink:"/classy/docs/getting-started/customizing-things/custom-optimizer"},next:{title:"train",permalink:"/classy/docs/reference-manual/cli/train"}},c={},p=[{value:"A Minimal Example",id:"a-minimal-example",level:2},{value:"Monitoring at Training Time",id:"monitoring-at-training-time",level:2},{value:"Swapping Evaluation Metric",id:"swapping-evaluation-metric",level:2}],u={toc:p};function m(t){let{components:e,...a}=t;return(0,i.kt)("wrapper",(0,n.Z)({},u,a,{components:e,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Adding a custom metric for evaluation is easy in ",(0,i.kt)("inlineCode",{parentName:"p"},"classy"),", and you can use it for both ",(0,i.kt)("inlineCode",{parentName:"p"},"classy evaluate")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"classy train")," (to monitor performance or, perhaps, even early-stop). To do this, you just need to:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Write your ",(0,i.kt)("em",{parentName:"li"},"Evaluation")," class")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"class Evaluation:\n    def __call__(\n        self,\n        path: str,\n        predicted_samples: List[ClassySample],\n    ) -> Dict:\n        raise NotImplementedError\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Write its config"),(0,i.kt)("li",{parentName:"ol"},"Train, specifying your evaluation ",(0,i.kt)("inlineCode",{parentName:"li"},"classy train [...] -c [...] evaluation=<your evaluation name>")),(0,i.kt)("li",{parentName:"ol"},"using ",(0,i.kt)("inlineCode",{parentName:"li"},"classy evaluate")," now prints your custom evaluation")),(0,i.kt)("h2",{id:"a-minimal-example"},"A Minimal Example"),(0,i.kt)("p",null,"As an example, imagine you want to use SpanF1 to evaluate your NER (Named Entity Recognition) system. First, you implement\nthe class:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="classy/evaluation/span.py"',title:'"classy/evaluation/span.py"'},'from datasets import load_metric\n\nclass SeqEvalSpanEvaluation(Evaluation):\n    def __init__(self):\n        self.backend_metric = load_metric("seqeval")\n\n    def __call__(\n        self,\n        predicted_samples: List[TokensSample],\n    ) -> Dict:\n\n        metric_out = self.backend_metric.compute(\n            predictions=[labels for _, labels in predicted_samples],\n            references=[sample.labels for sample, _ in predicted_samples],\n        )\n        p, r, f1 = metric_out["overall_precision"], metric_out["overall_recall"], metric_out["overall_f1"]\n\n        return {"precision": p, "recall": r, "f1": f1}\n')),(0,i.kt)("p",null,"We use here the SpanF1 metric implemented in the HuggingFace ",(0,i.kt)("em",{parentName:"p"},"datasets")," library (this is what ",(0,i.kt)("em",{parentName:"p"},'load_metric("seqeval")'),"\ndoes). Then, you write the corresponding config:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="configurations/evaluation/span.yaml"',title:'"configurations/evaluation/span.yaml"'},"_target_: 'classy.evaluation.span.SeqEvalSpanEvaluation'\n")),(0,i.kt)("p",null,"Finally, add this evaluation metric to your training configuration, train your model and automatically evaluate with\nyour metric:"),(0,i.kt)(r.Z,{mdxType:"ReactTermynal"},(0,i.kt)("span",{"data-ty":"input"},"classy train token DATA-PATH -n token -c evaluation=span"),(0,i.kt)("span",{"data-ty":"progress"}),(0,i.kt)("span",{"data-ty":!0},"Training completed"),(0,i.kt)("span",{"data-ty":"input"},"classy evaluate MODEL-PATH TEST-PATH"),(0,i.kt)("span",{"data-ty":"progress"}),(0,i.kt)("span",{"data-ty":!0},"* precision: 0.8746950156849076"),(0,i.kt)("span",{"data-ty":!0},"* recall: 0.8886331444759207"),(0,i.kt)("span",{"data-ty":!0},"* f1: 0.8816089935007905")),(0,i.kt)("h2",{id:"monitoring-at-training-time"},"Monitoring at Training Time"),(0,i.kt)("p",null,"As a matter of fact, most of the time you'll want to monitor your evaluation metric on some dataset (most likely, the validation)\nalso during training. You can achieve this as follows:"),(0,i.kt)(r.Z,{mdxType:"ReactTermynal"},(0,i.kt)("span",{"data-ty":"input"},"classy train token DATA-PATH -n token -c callbacks=evaluation evaluation=span"),(0,i.kt)("span",{"data-ty":"progress"}),(0,i.kt)("span",{"data-ty":!0},"Training completed")),(0,i.kt)("p",null),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"callbacks=evaluation")," is what does the magic. Behind the scenes, what is happening is that you are adding a callback\nwith the following config (which, obviously, you can modify either with ",(0,i.kt)("inlineCode",{parentName:"p"},"-c")," or via profile):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="configurations/callbacks/evaluation.yaml"',title:'"configurations/callbacks/evaluation.yaml"'},'- _target_: "classy.pl_callbacks.prediction.PredictionPLCallback"\n  path: null  # leave it to null to set it to validation path\n  prediction_dataset_conf: ${prediction.dataset}\n  on_result:\n    file_dumper:\n      _target_: "classy.pl_callbacks.prediction.FileDumperPredictionCallback"\n    evaluation:\n      _target_: "classy.pl_callbacks.prediction.EvaluationPredictionCallback"\n      evaluation: ${evaluation}\n  settings:\n    - name: "validation"\n      path: null  # leave it to null to set it to PredictionPLCallback.path\n      token_batch_size: 800\n      limit: 1000\n      prediction_param_conf_path: null\n      on_result:\n        - "file_dumper"\n        - "evaluation"\n')),(0,i.kt)("p",null,"Left as it is, this config tells ",(0,i.kt)("inlineCode",{parentName:"p"},"classy")," to use the model being trained to predict all samples in the validation dataset,\nand runs 2 callbacks on the resulting (sample, prediction) tuples:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"FileDumperPredictionCallback"),"; this callback dumps the (sample, prediction) tuples that your model predicts at each\nvalidation epoch in a dedicated folder in your experiment directory"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"EvaluationPredictionCallback")," (the actual magic); this callback evaluates the (sample, prediction) tuples with the\nevaluation metric you specified and logs the result")),(0,i.kt)("p",null,"More in detail, ",(0,i.kt)("em",{parentName:"p"},"PredictionPLCallback")," is a powerful class supporting quite the number of evaluation scenarios during\nyour training. It has 2 main arguments:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"on_result"),", a dictionary of (name, callback) pairs; each callback here is a ",(0,i.kt)("em",{parentName:"li"},"classy.pl_callbacks.prediction.PredictionCallback")," class"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"settings"),", a list of settings where model prediction should be performed, each made up of:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"name")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"path")," (containing the dataset you want to evaluate upon)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"token_batch_size"),", the token batch size you want to use (remember, no gradient computation here)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"limit"),", the maximum number of samples to be used (chosen as they occur in the dataset); set it to -1 to use all of them"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"prediction_param_conf_path"),", the path to the prediction params config file you want to use (leave it to null if not needed)"),(0,i.kt)("li",{parentName:"ul"},"(optionally) ",(0,i.kt)("em",{parentName:"li"},"on_result"),", a list containing the name of the on_result callbacks to want to launch on this setting; if not\nprovided, all callbacks will be used")))),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"You can use your metric for early-stopping as well! Just add\n",(0,i.kt)("inlineCode",{parentName:"p"},"-c [...] callbacks_monitor=<setting-name>-<name-of-metric-returned-in-evaluation-dict> callbacks_mode=<max-or-min>"),".\nFor instance, in our example, to early-stop on SpanF1 on the validation set,\nuse ",(0,i.kt)("inlineCode",{parentName:"p"},"-c [...] callbacks_monitor=validation-f1 callbacks_mode=max"))),(0,i.kt)("h2",{id:"swapping-evaluation-metric"},"Swapping Evaluation Metric"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"classy")," also supports changing the evaluation metric directly when using ",(0,i.kt)("inlineCode",{parentName:"p"},"classy evaluate"),", regardless of the config\nused in ",(0,i.kt)("inlineCode",{parentName:"p"},"classy train"),". To do so, you can use the the ",(0,i.kt)("inlineCode",{parentName:"p"},"--evaluation-config")," CLI parameter to ",(0,i.kt)("inlineCode",{parentName:"p"},"classy evaluate"),". This\nparameter specifies the configuration path (e.g. ",(0,i.kt)("em",{parentName:"p"},"configurations/evaluation/span.yaml"),") where the config of the desired\nevaluation metric is stored."),(0,i.kt)(r.Z,{mdxType:"ReactTermynal"},(0,i.kt)("span",{"data-ty":"input"},"classy train token DATA-PATH -n token"),(0,i.kt)("span",{"data-ty":"progress"}),(0,i.kt)("span",{"data-ty":!0},"Training completed"),(0,i.kt)("span",{"data-ty":"input"},"classy evaluate MODEL-PATH TEST-PATH"),(0,i.kt)("span",{"data-ty":"progress"}),(0,i.kt)("span",{"data-ty":!0},"# Evaluation with original training config"),(0,i.kt)("span",{"data-ty":!0},"[...]"),(0,i.kt)("span",{"data-ty":"input"},"classy evaluate MODEL-PATH TEST-PATH --evaluation-config configurations/evaluation/span.yaml"),(0,i.kt)("span",{"data-ty":"progress"}),(0,i.kt)("span",{"data-ty":!0},"* precision: 0.8746950156849076"),(0,i.kt)("span",{"data-ty":!0},"* recall: 0.8886331444759207"),(0,i.kt)("span",{"data-ty":!0},"* f1: 0.8816089935007905")),(0,i.kt)("p",null),(0,i.kt)("admonition",{type:"caution"},(0,i.kt)("p",{parentName:"admonition"},"Note that interpolation to other configs is currently not supported in this setting.")))}m.isMDXComponent=!0}}]);