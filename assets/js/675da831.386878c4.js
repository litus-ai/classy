"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9942],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>f});var r=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,r,s=function(e,t){if(null==e)return{};var a,r,s={},n=Object.keys(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var o=r.createContext({}),d=function(e){var t=r.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},c=function(e){var t=d(e.components);return r.createElement(o.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var a=e.components,s=e.mdxType,n=e.originalType,o=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),p=d(a),f=s,k=p["".concat(o,".").concat(f)]||p[f]||u[f]||n;return a?r.createElement(k,l(l({ref:t},c),{},{components:a})):r.createElement(k,l({ref:t},c))}));function f(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var n=a.length,l=new Array(n);l[0]=p;var i={};for(var o in t)hasOwnProperty.call(t,o)&&(i[o]=t[o]);i.originalType=e,i.mdxType="string"==typeof e?e:s,l[1]=i;for(var d=2;d<n;d++)l[d]=a[d];return r.createElement.apply(null,l)}return r.createElement.apply(null,a)}p.displayName="MDXCreateElement"},38:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>n,metadata:()=>i,toc:()=>d});var r=a(7462),s=(a(7294),a(3905));const n={title:"classy.data.dataset.hf.base",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},l=void 0,i={unversionedId:"api/data/dataset/hf/base",id:"api/data/dataset/hf/base",title:"classy.data.dataset.hf.base",description:"Classes",source:"@site/docs/api/data/dataset/hf/base.md",sourceDirName:"api/data/dataset/hf",slug:"/api/data/dataset/hf/base",permalink:"/classy/docs/api/data/dataset/hf/base",draft:!1,tags:[],version:"current",frontMatter:{title:"classy.data.dataset.hf.base",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},sidebar:"apiSidebar"},o={},d=[{value:"Classes",id:"clzs",level:2},{value:"HFBaseDataset",id:"HFBaseDataset",level:3},{value:"__init__",id:"HFBaseDataset-init",level:4}],c={toc:d};function u(e){let{components:t,...a}=e;return(0,s.kt)("wrapper",(0,r.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h2",{id:"clzs"},"Classes"),(0,s.kt)("div",{className:"api"},(0,s.kt)("h3",{id:"HFBaseDataset"},"HFBaseDataset"),(0,s.kt)("div",{className:"api__signature"},(0,s.kt)("p",null,"class ",(0,s.kt)("span",{className:"ident"},"HFBaseDataset"),"()"),(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#HFBaseDataset",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/base.py#L11-L49",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"An iterable Dataset."),(0,s.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,s.kt)("p",null,"All subclasses should overwrite :meth:",(0,s.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,s.kt)("p",null,"When a subclass is used with :class:",(0,s.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,s.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,s.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,s.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,s.kt)("code",null,"__iter__")," method or the :class:",(0,s.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,s.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,s.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,s.kt)("code",null,"__iter__"),"::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,s.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,s.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,s.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,s.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,s.kt)("code",null,"worker_init_fn"),"::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,s.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,s.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,s.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,s.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,s.kt)("details",null,(0,s.kt)("summary",null,"Subclasses (4)"),(0,s.kt)("div",null,(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{title:"HFQADataset",href:"/docs/api/data/dataset/hf/classification#HFQADataset"},"HFQADataset")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{title:"HFSequenceDataset",href:"/docs/api/data/dataset/hf/classification#HFSequenceDataset"},"HFSequenceDataset")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{title:"HFTokenDataset",href:"/docs/api/data/dataset/hf/classification#HFTokenDataset"},"HFTokenDataset")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{title:"HFGenerationBaseDataset",href:"/docs/api/data/dataset/hf/generation#HFGenerationBaseDataset"},"HFGenerationBaseDataset"))))),(0,s.kt)("h4",{id:"HFBaseDataset-init"},"_","_","init","_","_"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"__init__"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0transformer_model:\xa0str,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0additional_special_tokens:\xa0Optional[List[str]]\xa0=\xa0None,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0truncation:\xa0Union[bool,\xa0str]\xa0=\xa0False,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0max_length:\xa0int\xa0=\xa0-1,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,s.kt)("br",null),")",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#HFBaseDataset-init",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/base.py#L11-L49",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__description"}))))}u.isMDXComponent=!0}}]);