"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9665],{3905:(t,e,a)=>{a.d(e,{Zo:()=>c,kt:()=>u});var s=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function i(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(t);e&&(s=s.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,s)}return a}function n(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?i(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function l(t,e){if(null==t)return{};var a,s,r=function(t,e){if(null==t)return{};var a,s,r={},i=Object.keys(t);for(s=0;s<i.length;s++)a=i[s],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(t);for(s=0;s<i.length;s++)a=i[s],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var o=s.createContext({}),d=function(t){var e=s.useContext(o),a=e;return t&&(a="function"==typeof t?t(e):n(n({},e),t)),a},c=function(t){var e=d(t.components);return s.createElement(o.Provider,{value:e},t.children)},_={inlineCode:"code",wrapper:function(t){var e=t.children;return s.createElement(s.Fragment,{},e)}},k=s.forwardRef((function(t,e){var a=t.components,r=t.mdxType,i=t.originalType,o=t.parentName,c=l(t,["components","mdxType","originalType","parentName"]),k=d(a),u=r,f=k["".concat(o,".").concat(u)]||k[u]||_[u]||i;return a?s.createElement(f,n(n({ref:e},c),{},{components:a})):s.createElement(f,n({ref:e},c))}));function u(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var i=a.length,n=new Array(i);n[0]=k;var l={};for(var o in e)hasOwnProperty.call(e,o)&&(l[o]=e[o]);l.originalType=t,l.mdxType="string"==typeof t?t:r,n[1]=l;for(var d=2;d<i;d++)n[d]=a[d];return s.createElement.apply(null,n)}return s.createElement.apply(null,a)}k.displayName="MDXCreateElement"},7576:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>o,contentTitle:()=>n,default:()=>_,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var s=a(7462),r=(a(7294),a(3905));const i={title:"classy.data.dataset.hf.generation",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},n=void 0,l={unversionedId:"api/data/dataset/hf/generation",id:"api/data/dataset/hf/generation",title:"classy.data.dataset.hf.generation",description:"Classes",source:"@site/docs/api/data/dataset/hf/generation.md",sourceDirName:"api/data/dataset/hf",slug:"/api/data/dataset/hf/generation",permalink:"/classy/docs/api/data/dataset/hf/generation",draft:!1,tags:[],version:"current",frontMatter:{title:"classy.data.dataset.hf.generation",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},sidebar:"apiSidebar"},o={},d=[{value:"Classes",id:"clzs",level:2},{value:"BartHFGenerationDataset",id:"BartHFGenerationDataset",level:3},{value:"__init__",id:"BartHFGenerationDataset-init",level:4},{value:"dataset_iterator_func",id:"BartHFGenerationDataset-dataset_iterator_func",level:4},{value:"DecHFGenerationBaseDataset",id:"DecHFGenerationBaseDataset",level:3},{value:"__init__",id:"DecHFGenerationBaseDataset-init",level:4},{value:"get_batching_fields",id:"DecHFGenerationBaseDataset-get_batching_fields",level:4},{value:"group_elements_on_materializations",id:"DecHFGenerationBaseDataset-group_elements_on_materializations",level:4},{value:"EncDecHFGenerationBaseDataset",id:"EncDecHFGenerationBaseDataset",level:3},{value:"__init__",id:"EncDecHFGenerationBaseDataset-init",level:4},{value:"get_batching_fields",id:"EncDecHFGenerationBaseDataset-get_batching_fields",level:4},{value:"GPT2HFGenerationCataset",id:"GPT2HFGenerationCataset",level:3},{value:"__init__",id:"GPT2HFGenerationCataset-init",level:4},{value:"dataset_iterator_func",id:"GPT2HFGenerationCataset-dataset_iterator_func",level:4},{value:"HFGenerationBaseDataset",id:"HFGenerationBaseDataset",level:3},{value:"__init__",id:"HFGenerationBaseDataset-init",level:4},{value:"get_batching_fields",id:"HFGenerationBaseDataset-get_batching_fields",level:4},{value:"group_elements_on_materializations",id:"HFGenerationBaseDataset-group_elements_on_materializations",level:4},{value:"materialize_batches",id:"HFGenerationBaseDataset-materialize_batches",level:4},{value:"adapt_dataset_from",id:"HFGenerationBaseDataset-adapt_dataset_from",level:4},{value:"fit_vocabulary",id:"HFGenerationBaseDataset-fit_vocabulary",level:4},{value:"requires_vocab",id:"HFGenerationBaseDataset-requires_vocab",level:4},{value:"MBartHFGenerationDataset",id:"MBartHFGenerationDataset",level:3},{value:"__init__",id:"MBartHFGenerationDataset-init",level:4},{value:"dataset_iterator_func",id:"MBartHFGenerationDataset-dataset_iterator_func",level:4},{value:"group_elements_on_materializations",id:"MBartHFGenerationDataset-group_elements_on_materializations",level:4},{value:"T5HFGenerationDataset",id:"T5HFGenerationDataset",level:3},{value:"__init__",id:"T5HFGenerationDataset-init",level:4},{value:"dataset_iterator_func",id:"T5HFGenerationDataset-dataset_iterator_func",level:4}],c={toc:d};function _(t){let{components:e,...a}=t;return(0,r.kt)("wrapper",(0,s.Z)({},c,a,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"clzs"},"Classes"),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"BartHFGenerationDataset"},"BartHFGenerationDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"BartHFGenerationDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#BartHFGenerationDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L86-L152",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Subclasses (1)"),(0,r.kt)("div",null,(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"MBartHFGenerationDataset",href:"/docs/api/data/dataset/hf/generation#MBartHFGenerationDataset"},"MBartHFGenerationDataset"))))),(0,r.kt)("h4",{id:"BartHFGenerationDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#BartHFGenerationDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L86-L152",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"BartHFGenerationDataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#BartHFGenerationDataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L106-L152",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"DecHFGenerationBaseDataset"},"DecHFGenerationBaseDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"DecHFGenerationBaseDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#DecHFGenerationBaseDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L66-L83",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Subclasses (1)"),(0,r.kt)("div",null,(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"GPT2HFGenerationCataset",href:"/docs/api/data/dataset/hf/generation#GPT2HFGenerationCataset"},"GPT2HFGenerationCataset"))))),(0,r.kt)("h4",{id:"DecHFGenerationBaseDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#DecHFGenerationBaseDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L66-L83",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"DecHFGenerationBaseDataset-get_batching_fields"},"get_batching_fields"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"get_batching_fields"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0bool,",(0,r.kt)("br",null),") \u2011>\xa0Optional[List[str]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#DecHFGenerationBaseDataset-get_batching_fields",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L67-L68",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"DecHFGenerationBaseDataset-group_elements_on_materializations"},"group_elements_on_materializations"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"group_elements_on_materializations"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0dataset_elements:\xa0List[Dict[str,\xa0Any]],",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0bool,",(0,r.kt)("br",null),") \u2011>\xa0List[List[Dict[str,\xa0Any]]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#DecHFGenerationBaseDataset-group_elements_on_materializations",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L70-L83",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"EncDecHFGenerationBaseDataset"},"EncDecHFGenerationBaseDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"EncDecHFGenerationBaseDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#EncDecHFGenerationBaseDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L61-L63",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Subclasses (2)"),(0,r.kt)("div",null,(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"BartHFGenerationDataset",href:"/docs/api/data/dataset/hf/generation#BartHFGenerationDataset"},"BartHFGenerationDataset")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"T5HFGenerationDataset",href:"/docs/api/data/dataset/hf/generation#T5HFGenerationDataset"},"T5HFGenerationDataset"))))),(0,r.kt)("h4",{id:"EncDecHFGenerationBaseDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#EncDecHFGenerationBaseDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L61-L63",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"EncDecHFGenerationBaseDataset-get_batching_fields"},"get_batching_fields"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"get_batching_fields"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0bool,",(0,r.kt)("br",null),") \u2011>\xa0Optional[List[str]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#EncDecHFGenerationBaseDataset-get_batching_fields",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L62-L63",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"GPT2HFGenerationCataset"},"GPT2HFGenerationCataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"GPT2HFGenerationCataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#GPT2HFGenerationCataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L301-L373",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("h4",{id:"GPT2HFGenerationCataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#GPT2HFGenerationCataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L301-L373",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"GPT2HFGenerationCataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#GPT2HFGenerationCataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L317-L373",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"HFGenerationBaseDataset"},"HFGenerationBaseDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"HFGenerationBaseDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L18-L58",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("details",null,(0,r.kt)("summary",null,"Subclasses (2)"),(0,r.kt)("div",null,(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"DecHFGenerationBaseDataset",href:"/docs/api/data/dataset/hf/generation#DecHFGenerationBaseDataset"},"DecHFGenerationBaseDataset")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{title:"EncDecHFGenerationBaseDataset",href:"/docs/api/data/dataset/hf/generation#EncDecHFGenerationBaseDataset"},"EncDecHFGenerationBaseDataset"))))),(0,r.kt)("h4",{id:"HFGenerationBaseDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L18-L58",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFGenerationBaseDataset-get_batching_fields"},"get_batching_fields"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"get_batching_fields"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0bool,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-get_batching_fields",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L40-L41",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFGenerationBaseDataset-group_elements_on_materializations"},"group_elements_on_materializations"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"group_elements_on_materializations"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0dataset_elements:\xa0List[Dict[str,\xa0Any]],",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0bool,",(0,r.kt)("br",null),") \u2011>\xa0List[List[Dict[str,\xa0Any]]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-group_elements_on_materializations",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L51-L54",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFGenerationBaseDataset-materialize_batches"},"materialize_batches"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"materialize_batches"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0dataset_elements:\xa0List[Dict[str,\xa0Any]],",(0,r.kt)("br",null),") \u2011>\xa0Generator[Dict[str,\xa0Any],\xa0None,\xa0None]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-materialize_batches",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L43-L49",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFGenerationBaseDataset-adapt_dataset_from"},"adapt_dataset_from"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"adapt_dataset_from"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0training_dataset:\xa0omegaconf.dictconfig.DictConfig,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0setting:\xa0str,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-adapt_dataset_from",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L27-L32",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFGenerationBaseDataset-fit_vocabulary"},"fit_vocabulary"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"fit_vocabulary"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0samples:\xa0Iterator[",(0,r.kt)("a",{title:"ClassySample",href:"/docs/api/data/data_drivers#ClassySample"},"ClassySample"),"],",(0,r.kt)("br",null),") \u2011>\xa0",(0,r.kt)("a",{title:"Vocabulary",href:"/docs/api/utils/vocabulary#Vocabulary"},"Vocabulary"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-fit_vocabulary",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L23-L25",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"HFGenerationBaseDataset-requires_vocab"},"requires_vocab"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("div",{className:"annotation"},"@classmethod"),"def ",(0,r.kt)("span",{className:"ident"},"requires_vocab"),"() \u2011>\xa0bool",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#HFGenerationBaseDataset-requires_vocab",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L19-L21",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"MBartHFGenerationDataset"},"MBartHFGenerationDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"MBartHFGenerationDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#MBartHFGenerationDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L155-L235",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("h4",{id:"MBartHFGenerationDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#MBartHFGenerationDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L155-L235",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"MBartHFGenerationDataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#MBartHFGenerationDataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L162-L222",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"MBartHFGenerationDataset-group_elements_on_materializations"},"group_elements_on_materializations"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"group_elements_on_materializations"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0dataset_elements:\xa0List[Dict[str,\xa0Any]],",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0bool,",(0,r.kt)("br",null),") \u2011>\xa0List[List[Dict[str,\xa0Any]]]",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#MBartHFGenerationDataset-group_elements_on_materializations",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L224-L235",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))),(0,r.kt)("div",{className:"api"},(0,r.kt)("h3",{id:"T5HFGenerationDataset"},"T5HFGenerationDataset"),(0,r.kt)("div",{className:"api__signature"},(0,r.kt)("p",null,"class ",(0,r.kt)("span",{className:"ident"},"T5HFGenerationDataset"),"()"),(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#T5HFGenerationDataset",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L238-L298",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"},(0,r.kt)("p",null,"An iterable Dataset."),(0,r.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,r.kt)("p",null,"All subclasses should overwrite :meth:",(0,r.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,r.kt)("p",null,"When a subclass is used with :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,r.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,r.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,r.kt)("code",null,"__iter__")," method or the :class:",(0,r.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,r.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,r.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,r.kt)("code",null,"__iter__"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,r.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,r.kt)("code",null,"worker_init_fn"),"::"),(0,r.kt)("pre",null,(0,r.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,r.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,r.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,r.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,r.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,r.kt)("h4",{id:"T5HFGenerationDataset-init"},"_","_","init","_","_"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"__init__"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0teacher_forcing:\xa0Optional[bool]\xa0=\xa0True,",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#T5HFGenerationDataset-init",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L238-L298",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__description"}),(0,r.kt)("div",{className:"api"},(0,r.kt)("h4",{id:"T5HFGenerationDataset-dataset_iterator_func"},"dataset_iterator_func"),(0,r.kt)("div",{className:"api__signature"},"def ",(0,r.kt)("span",{className:"ident"},"dataset_iterator_func"),"(",(0,r.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,r.kt)("br",null),")",(0,r.kt)("div",{className:"links-div"},(0,r.kt)("a",{href:"#T5HFGenerationDataset-dataset_iterator_func",className:"direct-link"},"#"),(0,r.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/2ade6cc06ca6ed0708984f5c3ab3ddb7a2d57759/classy/data/dataset/hf/generation.py#L255-L298",className:"git-link"},"#"))),(0,r.kt)("div",{className:"api__body"},(0,r.kt)("div",{className:"api__description"}))))))}_.isMDXComponent=!0}}]);