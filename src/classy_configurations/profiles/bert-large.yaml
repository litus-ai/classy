supported_tasks:
  - qa
  - sentence-pair
  - sequence
  - token

# global params
transformer_model: bert-large-cased

# trainer
training:
  pl_trainer:
    accumulate_grad_batches: 1
    val_check_interval: 1.0
    max_steps: 100_000

# MODEL PARAMS
model:
  optim_conf:
    _target_: classy.optim.factories.AdamWWithWarmupFactory
    lr: 3e-5
    warmup_steps: 5000
    total_steps: ${training.pl_trainer.max_steps}
    weight_decay: 0.01
    no_decay_params:
      - bias
      - LayerNorm.weight
