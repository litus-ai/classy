supported_tasks:
  - qa
  - sentence-pair
  - sequence
  - token

# global params
transformer_model: microsoft/deberta-large

# trainer
training:
  pl_trainer:
    accumulate_grad_batches: 1
    gradient_clip_val: 1.0
    val_check_interval: 1.0
    max_steps: 100_000

# MODEL PARAMS
model:
  optim_conf:
    _target_: classy.optim.factories.RAdamFactory
    lr: 5e-6
    weight_decay: 0.01
    no_decay_params:
      - bias
      - LayerNorm.weight
